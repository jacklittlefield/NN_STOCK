{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Report \n",
    "### Jack Littlefield - Capstone\n",
    "\n",
    "## Problem Statement:\n",
    "\n",
    "##### Throughout the history of the stock market, people have been trying to accurately price stocks and make money thereafter. There are two main types of modeling equity prices. The first one, fundamental modeling, uses current data, such as P/E ratios and EPS, to accurately price stocks. For my capstone project, I focused on technical research. Technical research uses tends in moving averages, standard deviations, and past historical momentum of a stock. I decided to build a neural network using time series equity data to see if I could accurately predict the price of a stock 5 days out. \n",
    "\n",
    "## Data Collection:\n",
    "\n",
    "##### My original plan was to scrape the data using beautiful soup and Robinhood, the stock trading and investing app. Their API allows for scraping of stock data one year in the past. However, I realized I wanted a longer window than the one year provided by a few APIs. So, I used yahoo finance to get csv’s that go back 20-30 years of the stocks history. Users wishing to replicate my findings may also go to Yahoo! Finance and download the necessary data. I am also using an interest rate feature, and this data was collected from the St. Louis Federal Reserve Bank website. \n",
    "\n",
    "##### I am also going to build a dashboard that does use beautiful soup to get the ratios and what they will be based on my stock prediction. \n",
    "\n",
    "## Data Exploration:\n",
    "\n",
    "##### Stock data is sparse when attempting to explore the data. Luckily, the data came in clean, so I did not have to fill missing values manually. However, there are some things that I needed to do to make sure my data is all ready for a model. The first thing I did was to implement Bollinger bands into charts to help me visualize the data.  While this band can take many forms, mine is set up as two standard deviations from the 20-day rolling mean. If my actual closing price crosses one of these lines, I will be able to visualize this using the Bollinger bands I graphed. \n",
    "\n",
    "![Image description](BollingerBands.png)\n",
    "\n",
    "\n",
    "##### This graph is a great illustration of my Bollinger bands. The blue line is the close price, and the orange and green lines are plus and minus 2 standard deviations from the 20 day moving average of the close price. \n",
    "\n",
    "\n",
    "\n",
    "##### I also individually plotted the different features to determine what I wanted to use in my model. For example, I graphed volume of trades. However, volume did not have any discernable impact on price, so I did not include volume in my final model. \n",
    "\n",
    "##### One important part of my data exploration was focusing on the time series aspect of the historical prices. So, I ran a partial auto correlation graph on the stock. In short, this tells me whether there are points in history that are correlated with the current price, without taking in historical data. \n",
    "\n",
    " \n",
    "\n",
    "\n",
    "##### In the above graph are days back that have a positive correlation to today’s price. Some of those days, such as the 60th day are used as features in my model. \n",
    "\n",
    "\n",
    "## Model Building:\n",
    "\n",
    "##### In time series data, training and testing datasets are done on a date. Selecting a date will vary depending on the stock. For General Electric (GE) I chose 2015 as the separation between my training and testing data. For Blue Apron (APRN), I chose June 2018. The reason for the massive difference here is due to the length of time each stock has been on the market. Blue Apron, for example, IPO'd last summer, and GE has been around for a while. Here is a visualization of my train test split for APRN. Price is on the Y-Axis, and the orange part of the line represents my holdout data.  \n",
    "![Blue Apron Train Test Split](APRN TTS.png)\n",
    "\n",
    "##### When first thinking about what type of model to build, I thought about which ones are best for time series data. An obvious choice whould be an AR1 model. However, this is simply just a moving average, and does not remember things in the past, espcially about how they related to the interaction between the 60th day, in the risk free rate at the time. I built an SVR model, and it worked okay in a few circumstances, but overall, I wanted to build a Neural Network. \n",
    "\n",
    "##### In building my neural network, I minimised mean squared error. In determining stock price, one wants to be as close as possible. I also tested over different activation functions. In the end, Relu minimized my loss the most. \n",
    "\n",
    "## Results:\n",
    "\n",
    "##### My R2 score for APRN is 96% in the holdout data. The following graph shoes the validation loss of my training and testing data.\n",
    "![ValLoss](ValLoss.png)\n",
    "\n",
    "##### How do my predictions match up with the actual price on a given day? \n",
    "![ValLoss](PredsAct.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
